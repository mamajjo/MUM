{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "!pip install scipy numpy matplotlib pandas sklearn tabulate seaborn folium geopy geopandas requests> /dev/null\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import sys\n",
    "# importlib.reload(sys.modules['statistics.company'])\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, learning_curve, permutation_test_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, plot_roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./module-dir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_provider.provider import get_dataset\n",
    "from dataset_provider.config import create_global_config\n",
    "cfg = create_global_config()\n",
    "df = get_dataset(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot company sizes and salary ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statistics.company import plot_stats\n",
    "plot_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statistics.percentage import count_percentage\n",
    "count_percentage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from heat_maps.density_map import density_heat_map\n",
    "density_heat_map(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get province data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from heat_maps.salary_map import show_map\n",
    "show_map(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df = df.copy()\n",
    "for col in [\"city\", \"Unnamed: 0\", \"title\", \"company_size\", \"country_code\", \"marker_icon\", \"company_name\",\n",
    "            \"latitude\", \"longitude\", \"salary_currency\", \"published_at\", \"remote_interview\", \"id\",\n",
    "            \"Vert.x\", \"skills\", \"-\"]:\n",
    "                cl_df=cl_df.drop(col, axis=1)\n",
    "print(cl_df.columns)\n",
    "cols_for_encoding = [c for c in cl_df.select_dtypes(include=['object']).copy().columns if c != \"experience_level\"]\n",
    "print(cols_for_encoding)\n",
    "cl_df = pd.get_dummies(cl_df, columns=cols_for_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cl_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_col_to_end(df, col):\n",
    "    cols_at_end = [col]\n",
    "    return df[[c for c in df if c not in cols_at_end] \n",
    "            + [c for c in cols_at_end if c in df]]\n",
    "\n",
    "cl_df = move_col_to_end(cl_df, \"experience_level\")\n",
    "array = cl_df.values\n",
    "x = array[:,0:len(cl_df.columns)-1]\n",
    "y = array[:,len(cl_df.columns)-1]\n",
    "x = normalize(x)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "#na podstawie x i y otrzymujemy tablice testowe i wynikowe\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x,y, test_size=cfg.test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.extend([\n",
    "    ('KNN', KNeighborsClassifier(), 0),\n",
    "    ('CART', DecisionTreeClassifier(), 1),\n",
    "    ('NB', GaussianNB(), 2),\n",
    "    ('SVM', SVC(gamma='auto'), 3),\n",
    "    ('MLP', MLPClassifier(alpha=1e-5, hidden_layer_sizes=(50,10), max_iter=5000), 4)\n",
    "    ])\n",
    "def plot_show():\n",
    "    pyplot.draw()\n",
    "    pyplot.pause(0.1)\n",
    "\n",
    "def get_specificity(y_validate, y_predicted):\n",
    "    cnf_matrix = confusion_matrix(y_validate, y_predicted)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    \n",
    "    FP = FP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "        \n",
    "    return np.mean(TN/(TN+FP))\n",
    "    \n",
    "def get_learning_curve(classification_model, training_set_enlarging_step=10):\n",
    "    train_sizes = np.linspace(0.1, 1, training_set_enlarging_step)\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        estimator = classification_model,\n",
    "        X = x,\n",
    "        y = y, \n",
    "        train_sizes = train_sizes,\n",
    "        cv = 5,\n",
    "        scoring = 'accuracy')\n",
    "    return train_sizes, train_scores, validation_scores\n",
    "\n",
    "def plot_learning_curve(model,name):\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "        get_learning_curve(model)\n",
    "\n",
    "    plt.style.use('seaborn')\n",
    "    plt.plot(train_sizes, -train_scores.mean(axis = 1), color= 'red', label = 'Training error')\n",
    "    plt.plot(train_sizes, -test_scores.mean(axis = 1), color= 'navy',label = 'Validation error')\n",
    "    plt.ylabel('Accuracy', fontsize = 14)\n",
    "    plt.xlabel('Training set size', fontsize = 14)\n",
    "    plt.title('Learning curves for a %s' % name, fontsize = 18, y = 1.03)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def compare_algorithms(results, names):\n",
    "    fig = pyplot.figure()\n",
    "    fig.suptitle(\"Algorithm Comparison\")\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_title(\"Algorithm Comparison\")\n",
    "    ax.boxplot(results, labels=names)\n",
    "    plot_show()\n",
    "\n",
    "def print_scores(cv_results, predictions):\n",
    "    print('\\nMean %f' % cv_results.mean())\n",
    "    print('STD %f' % cv_results.std())\n",
    "    print('\\nConfusion matrix:')\n",
    "    print(confusion_matrix(y_validation, predictions))\n",
    "    print('\\nAccuracy %f' % accuracy_score(y_validation, predictions))\n",
    "    print('Precision %f' % precision_score(y_validation, predictions, average = 'weighted'))\n",
    "    print('Recall %f' % recall_score(y_validation, predictions, average = 'weighted'))\n",
    "    print('Specificity %f' % get_specificity(y_validation, predictions))\n",
    "    print('\\nClassification report:')\n",
    "    print(classification_report(y_validation, predictions))\n",
    "\n",
    "def plot_roc_curves():\n",
    "    fig = pyplot.figure()\n",
    "    ax = plt.gca()\n",
    "    for name, model, subplot_row in models:\n",
    "        rfc_disp = plot_roc_curve(model, x_validation, \n",
    "                                  y_validation, ax=ax, alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "def accuracySignificancy(model, x_train, y_train, cv):\n",
    "    fig = pyplot.figure()\n",
    "    fig.suptitle(\"Estimating accuracy score's statistical significancy\")\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    n_classes = np.unique(y_train).size\n",
    "    score, permutation_scores, pvalue = permutation_test_score(model, x_train, y_train, scoring=\"accuracy\", cv=cv, n_permutations=100)\n",
    "    print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))\n",
    "    # View histogram of permutation scores\n",
    "    ax.hist(permutation_scores, 20, label='Permutation scores',\n",
    "             edgecolor='black')\n",
    "    ylim = plt.ylim()\n",
    "    ax.set_xlabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_per_dataset():\n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model, subplot_row in models:\n",
    "            print(f\"---------------------------\\nRunning classification for: {name}\")\n",
    "            kfold = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "            cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "            accuracySignificancy(model, x_train, y_train, kfold)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "\n",
    "            # Make predictions on validation dataset\n",
    "            model.fit(x_train, y_train)\n",
    "            predictions = model.predict(x_validation)\n",
    "            \n",
    "            print_scores(cv_results, predictions)\n",
    "            plot_learning_curve(model,name)\n",
    "\n",
    "    # Compare Algorithms - ROC etc\n",
    "    compare_algorithms(results, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_per_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cl_df = df.copy()\n",
    "for col in [\"salary_to\", \"city\", \"Unnamed: 0\", \"title\", \"company_size\", \"country_code\", \"marker_icon\", \"company_name\",\n",
    "            \"latitude\", \"longitude\", \"salary_currency\", \"published_at\", \"remote_interview\", \"id\",\n",
    "            \"Vert.x\", \"skills\", \"-\"]:\n",
    "                cl_df=cl_df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_encoding = [c for c in cl_df.select_dtypes(include=['object', 'boolean']).copy().columns]\n",
    "cl_df = pd.get_dummies(cl_df, columns=cols_for_encoding)\n",
    "cl_df = move_col_to_end(cl_df, \"salary_from\")\n",
    "array = cl_df.values\n",
    "x = array[:, 0:len(cl_df.columns)-1]\n",
    "y = array[:, len(cl_df.columns)-1]\n",
    "x = normalize(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_test_split(x,y, test_size=cfg.test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 56)\n",
    "# x = pca.fit_transform(x)\n",
    "# print(len(x[0]))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  # doctest: +SKIP\n",
    "scaler.fit(x_train)  # doctest: +SKIP\n",
    "x_train = scaler.transform(x_train)  # doctest: +SKIP\n",
    "x_validation = scaler.transform(x_validation)  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge, Lasso, ElasticNet, Lars, OrthogonalMatchingPursuit, SGDRegressor, ARDRegression\n",
    "from sklearn.linear_model import MultiTaskLasso, MultiTaskLassoCV, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def plot_data(axes, title, x_train, y_train, x_test, y_test):\n",
    "    axes.set_title(title)\n",
    "    axes.scatter(x_train[:,0], y_train, color=\"green\"),\n",
    "    axes.plot(x_test, y_test, color=\"red\", linewidth=1)\n",
    "    \n",
    "models = [\n",
    "#     LinearRegression(),\n",
    "    HuberRegressor(alpha=0.00001, epsilon=1.6, max_iter=5000),\n",
    "    BayesianRidge(),\n",
    "#     SGDRegressor(),\n",
    "#     ARDRegression(),\n",
    "#     TheilSenRegressor(),\n",
    "#     MultiTaskLasso(),\n",
    "#     MultiTaskLassoCV(),\n",
    "#     OrthogonalMatchingPursuit(),\n",
    "#     Lars(),\n",
    "#     MLPRegressor(hidden_layer_sizes=(9), solver='lbfgs', tol=1e-6, learning_rate='adaptive', max_iter=3000, n_iter_no_change=100),\n",
    "#     MLPRegressor(hidden_layer_sizes=(9), alpha=1e-8, solver='lbfgs', tol=1e-6, learning_rate='adaptive', max_iter=3000, n_iter_no_change=100),\n",
    "#     MLPRegressor(hidden_layer_sizes=(9), alpha=1, solver='lbfgs', tol=1e-6, learning_rate='adaptive', max_iter=3000, n_iter_no_change=100),\n",
    "#     MLPRegressor(hidden_layer_sizes=(16), solver='sgd', learning_rate='adaptive', max_iter=500, n_iter_no_change=100),\n",
    "#     SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1),\n",
    "#     SVR(kernel='poly', C=100, gamma='scale', degree=3, epsilon=.1, coef0=1),\n",
    "#     SVR(kernel='linear', C=100, gamma='scale'),\n",
    "#     SVR(kernel='rbf'),\n",
    "#     SVR(kernel='poly', degree=60, gamma='scale'),\n",
    "#     SVR(kernel='linear', gamma='scale'),    \n",
    "#     SVR(kernel='sigmoid', gamma='scale'),\n",
    "#     SVR(kernel='precomputed', gamma='auto'),\n",
    "    Lasso(alpha=0.1, max_iter=1000_00, random_state=True),\n",
    "    ElasticNet(alpha=0.01, max_iter=1000_00, random_state=True)]\n",
    "\n",
    "# f, axarr = plt.subplots(len(models), sharex=True, sharey=True,figsize=(12,12))\n",
    "for y, model in enumerate(models):\n",
    "        y_test = model.fit(x_train, y_train).predict(x_validation)\n",
    "#         plot_data(axarr[y], type(model).__name__, x_train, y_train, x_validation, y_test)\n",
    "        print(f\"{type(model).__name__} MAE: {mean_absolute_error(y_validation, y_test, )} | MSE: {mean_squared_error(y_validation, y_test, )}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, y in enumerate(y_validation):\n",
    "    print(f\"predicted: {y_test[i]} original: {y_validation[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(__doc__)\n",
    "\n",
    "\n",
    "# # Code source: GaÃ«l Varoquaux\n",
    "# # Modified for documentation by Jaques Grobler\n",
    "# # License: BSD 3 clause\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "# from sklearn import datasets\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# pca = PCA()\n",
    "# bayesian = BayesianRidge() #(max_iter=10000, tol=0.1)\n",
    "# pipe = Pipeline(steps=[('pca', pca), ('bayesian', bayesian)])\n",
    "# param_grid = {\n",
    "#     'pca__n_components': [5, 15, 30, 45, 64]\n",
    "# }\n",
    "# search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "# search.fit(x, y)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)\n",
    "\n",
    "# # Plot the PCA spectrum\n",
    "# pca.fit(x)\n",
    "\n",
    "# fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "# ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "#          pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "# ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "# ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "#             linestyle=':', label='n_components chosen')\n",
    "# ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# # For each number of components, find the best classifier results\n",
    "# results = pd.DataFrame(search.cv_results_)\n",
    "# components_col = 'param_pca__n_components'\n",
    "# best_clfs = results.groupby(components_col).apply(\n",
    "#     lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "# best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "#                legend=False, ax=ax1)\n",
    "# ax1.set_xlabel('n_components')\n",
    "\n",
    "# plt.xlim(-1, 70)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
