{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "!pip install scipy numpy matplotlib pandas sklearn tabulate seaborn jupyterthemes folium geopy geopandas requests> /dev/null\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from geopy import Nominatim\n",
    "from pandas import read_csv, read_json\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, learning_curve, permutation_test_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, plot_roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import json\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import urllib.request \n",
    "from tabulate import tabulate\n",
    "import requests\n",
    "import webbrowser\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import geopandas as gpd\n",
    "from IPython.display import display, HTML\n",
    "# notebook styles\n",
    "# from jupyterthemes import jtplot\n",
    "# !jt -t onedork\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "# jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dataset configuration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, dataSourceRaw, dataSourceMapped, useOnlineData, test_size, n_splits, should_describe_data):\n",
    "        self.dataSourceRaw = dataSourceRaw\n",
    "        self.dataSourceMapped = dataSourceMapped\n",
    "        self.useOnlineData = useOnlineData\n",
    "        self.test_size = test_size\n",
    "        self.n_splits = n_splits\n",
    "        self.should_describe_data = should_describe_data\n",
    "\n",
    "def as_config(dict, dataSetName):\n",
    "    return Config(\n",
    "        dict[dataSetName]['dataSourceRaw'],\n",
    "        dict[dataSetName]['dataSourceMapped'],\n",
    "        dict[dataSetName]['useOnlineData'],\n",
    "        dict[dataSetName]['test_size'],\n",
    "        dict[dataSetName]['n_splits'],\n",
    "        dict[dataSetName]['should_describe_data'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json_config = \"\"\"\n",
    "{\n",
    "    \"it_data\":{\n",
    "        \"dataSourceRaw\": \"./data/joinit_data.json\",\n",
    "        \"dataSourceMapped\": \"./data/joinit_data.csv\",\n",
    "        \"useOnlineData\": true,\n",
    "        \"test_size\": 0.2,\n",
    "        \"n_splits\": 10,\n",
    "        \"should_describe_data\": true\n",
    "    }\n",
    "}\"\"\"\n",
    "curr_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dataset load</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_info(api_url = 'https://justjoin.it/api/offers'):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content.decode('utf-8'))\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and map skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_columns_to_csv(dataset):\n",
    "    skill_map = read_csv('./data/skills_mapped.csv', sep=\":\")\n",
    "    unique_skill_columns = (skill_map['mapping'].unique())\n",
    "\n",
    "    for skill in unique_skill_columns:\n",
    "        dataset[str(skill)] = 0\n",
    "    result_df = dataset\n",
    "    for index, row in dataset.iterrows():\n",
    "        skill_dict = row['skills']\n",
    "        for skill_level_tuple in skill_dict:\n",
    "            name = skill_level_tuple['name']\n",
    "            mapped_val = np.where(skill_map['Skill'] == name)\n",
    "            if len(mapped_val) or len(mapped_val[0]) is 0:\n",
    "                # TEMPORARY HACK\n",
    "                continue\n",
    "            name_index_in_map = np.where(skill_map['Skill'] == name)[0][0]\n",
    "            name = skill_map.iloc[name_index_in_map]['mapping']\n",
    "            if not name == '-':\n",
    "                if row[name] == 0:\n",
    "                    row[name] = skill_level_tuple['level']\n",
    "                elif not row[name] == 0:\n",
    "                    row[name] = row[name] if row[name] >= skill_level_tuple['level'] else skill_level_tuple['level']\n",
    "        result_df.loc[index] = row\n",
    "    result_df.to_csv(cfg.dataSourceMapped)\n",
    "\n",
    "def find_exchange_rate(code, table):\n",
    "    return next(filter(lambda c: c[\"code\"]==code, table[0][\"rates\"]))[\"mid\"]\n",
    "def take_only_country_translate_currency(df):\n",
    "    curr_table = None\n",
    "    uniq_countries = df['country_code'].nunique()\n",
    "    uniq_currencies = df['salary_currency'].nunique()\n",
    "    if uniq_countries != 1 or uniq_currencies != 1:\n",
    "        print(f\"Found {uniq_countries} countries and {uniq_currencies} currencies!\")\n",
    "        print(f\"Dropping foreign countries and translating currencies...\")\n",
    "        df = df.loc[df[\"country_code\"] == \"PL\"]\n",
    "        if not curr_table:\n",
    "            with urllib.request.urlopen(\"https://api.nbp.pl/api/exchangerates/tables/a/?format=json\") as url:\n",
    "                curr_table = json.loads(url.read().decode())\n",
    "        to_translate = df.loc[df[\"salary_currency\"] != \"pln\"]\n",
    "        for curr in to_translate.salary_currency.unique():\n",
    "            ex_rate = find_exchange_rate(curr.upper(), curr_table)\n",
    "            df.loc[df[\"salary_currency\"] == curr] = df.loc[df[\"salary_currency\"] == curr].apply(lambda x: x*ex_rate if x.name in [\"salary_from\", \"salary_to\"] else (\"pln\" if x.name == \"salary_currency\" else x))\n",
    "        print(f\"Unique countries: {df['country_code'].nunique()}, currencies: {df['salary_currency'].nunique()}, observations: {df.shape[0]}\")\n",
    "    return df\n",
    "    \n",
    "def read_dataset():\n",
    "    cfg = as_config(json.loads(json_config), 'it_data')\n",
    "    if cfg.useOnlineData:\n",
    "        jjit_json = get_jobs_info()\n",
    "        dataset = json_normalize(jjit_json)\n",
    "    else:\n",
    "        dataset = read_json(cfg.dataSourceRaw)\n",
    "    return read_csv(cfg.dataSourceMapped)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.drop(columns=[\"street\", \"address_text\", \"company_url\", \"company_logo_url\"])\n",
    "    # Save no-salary observations to separate dataframe: \"salaryless_df\"\n",
    "    global salaryless_df\n",
    "    salaryless_df = df.loc[((df.salary_currency.isnull()) | (df.salary_from.isnull()))]\n",
    "    df = df.loc[((df.salary_currency.notnull()) & (df.salary_from.notnull()))]\n",
    "    print(f\"Found {salaryless_df.shape[0]} job ads without salary range or currency\")\n",
    "    if cfg.should_describe_data:    \n",
    "        print(df.shape)\n",
    "        print(df[[\"salary_from\", \"salary_to\"]].describe())\n",
    "    df = take_only_country_translate_currency(df)\n",
    "    return df\n",
    "    \n",
    "def get_dataset():\n",
    "    return preprocess_data(read_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Load (and describe) dataset, create test and train datasets</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot company sizes and salary ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setAxesRanges(axes):\n",
    "    for a in axes:\n",
    "        start, end = a.get_ylim()\n",
    "        a.yaxis.set_ticks(np.arange(start, end, (end-start)/4))\n",
    "        a.set_ylim(top=end*1.2)\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "def transform_company_size(val):\n",
    "    val = val.replace('+', \"\").replace(\"<\", \"\").replace(\">\", \"\").replace(\" \", \"\")\n",
    "    if \"-\" in val:\n",
    "        ran = val.split(\"-\", 1)\n",
    "        if '' in ran:\n",
    "            return int(ran[0]) if ran[1] == \"\" else int(ran[1])\n",
    "        else:\n",
    "            return (int(ran[0])+int(ran[1]))/2\n",
    "    else:\n",
    "        return int(val)\n",
    "X = df[\"company_size\"].copy().values\n",
    "X = [transform_company_size(x) for x in X if hasNumbers(x)]\n",
    "first_X = [x for x in X if x < 1000]\n",
    "second_X = [x for x in X if x >= 1000 and x < 5000]\n",
    "third_X = [x for x in X if x >= 5000]\n",
    "\n",
    "salary_threshold = 80_000\n",
    "too_high_to_plot_count = df[df.salary_to>=salary_threshold].size\n",
    "if too_high_to_plot_count > 0:\n",
    "    print(f\"Found {too_high_to_plot_count} jobs with salary over {salary_threshold}, which won't be taken into account on plots below.\")\n",
    "sns.set(color_codes=True)\n",
    "f, axes = plt.subplots(3, 1)\n",
    "sns.distplot(first_X, ax=axes[0], kde=False, hist=True);\n",
    "sns.distplot(second_X, ax=axes[1], kde=False, hist=True);\n",
    "sns.distplot(third_X, ax=axes[2], kde=False, hist=True);\n",
    "f.tight_layout()\n",
    "setAxesRanges(axes)\n",
    "sns.distplot(df.copy().assign(Spread=lambda df: 100*(df.salary_to-df.salary_from)/((df.salary_to+df.salary_from)/2)).Spread, kde=False, hist=True)\n",
    "with sns.axes_style(\"white\"):\n",
    "    without_outlier=df.copy()[df.salary_to<salary_threshold]\n",
    "    sns.jointplot(x=without_outlier.salary_from, y=without_outlier.salary_to, kind=\"hex\");\n",
    "    sns.jointplot(x=without_outlier.salary_from, y=without_outlier.salary_to, data=df, kind=\"kde\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_percentage():\n",
    "    experience_level_count = df['experience_level'].value_counts()\n",
    "    experience_level_percentage = df['experience_level'].value_counts(normalize=True)\n",
    "    \n",
    "    \n",
    "    marker_icon_count = df['marker_icon'].value_counts()\n",
    "    marker_icon_percentage = df['marker_icon'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "    \n",
    "    \n",
    "    employment_type_count = df['employment_type'].value_counts()\n",
    "    employment_type_percentage = df['employment_type'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "    \n",
    "    df1 = pd.DataFrame({'Count': marker_icon_count, 'Percentage': marker_icon_percentage})\n",
    "    df2 = pd.DataFrame({'Count': experience_level_count, 'Percentage': experience_level_percentage.mul(100).round(1).astype(str) + '%'})\n",
    "    df3 = pd.DataFrame({'Count': employment_type_count, 'Percentage': employment_type_percentage})\n",
    "    display(df1)\n",
    "    df1.copy()[df1.Count>=25].plot.pie(y='Count', legend=False, figsize=(10, 10), autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    display(df2)\n",
    "    df2.plot.pie(y='Count', figsize=(5, 5), legend=False, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    display(df3)\n",
    "    df3.plot.pie(y='Count', figsize=(5, 5), legend=False, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "count_percentage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_column_name = 'lat'\n",
    "lon_column_name = 'lon'\n",
    "numerical_column_name = 'no'\n",
    "province_column_name = 'province'\n",
    "province_id_column_name = 'province_id'\n",
    "city_column_name = 'city'\n",
    "id_column_name = 'id'\n",
    "country_code_column_name = 'country_code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Coordinates load</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_province_from_address (address_text, province_string = \"województwo \",\n",
    "                               split_sign=\",\"):\n",
    "    province = address_text.partition(province_string)[2] \n",
    "    province = province.partition(split_sign)[0] \n",
    "    \n",
    "    return province\n",
    "    \n",
    "def fill_geo_info(places_data, loc_column_name, fill_province = False):\n",
    "    locator = Nominatim(user_agent=\"myGeocoder\")\n",
    "    \n",
    "    for i, row in places_data.iterrows():\n",
    "        location = locator.geocode(row[loc_column_name])\n",
    "        if location is not None:\n",
    "            places_data.loc[i,lat_column_name] = location.latitude\n",
    "            places_data.loc[i,lon_column_name] = location.longitude\n",
    "            if fill_province:\n",
    "                places_data.loc[i, province_column_name] = get_province_from_address(\n",
    "                location.address)\n",
    "        else:\n",
    "            print(f\"Could not find latitude/longitude for {row[loc_column_name]}\")\n",
    "            \n",
    "    return places_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Print heat map methods</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_heat_map(places_data, file_path = 'heat_map.html', \n",
    "                   heat_column_name = numerical_column_name):\n",
    "\n",
    "    places_len = len(places_data)\n",
    "    \n",
    "    lat = np.array(places_data[lat_column_name][0:places_len])\n",
    "    lon = np.array(places_data[lon_column_name][0:places_len])\n",
    "    no = np.array(places_data[heat_column_name][0:places_len],dtype=float)\n",
    "    data = [[lat[i],lon[i],no[i]] for i in range(places_len)] \n",
    "    \n",
    "    #location is the center location, draw a Map, and start zooming is 6 times.\n",
    "    map_osm = folium.Map(location=[lat.mean(),lon.mean()],zoom_start=6,control_scale=True)\n",
    "    HeatMap(data).add_to(map_osm) # Add heat map to the created map\n",
    "    display(map_osm)\n",
    "    map_osm.save(file_path) # Save as html file\n",
    "    webbrowser.open(file_path) # Default browser open\n",
    "\n",
    "def print_province_heat_map(province_data = None, heat_column_name = numerical_column_name,\n",
    "                            province_key_column_name = province_id_column_name,\n",
    "                            file_path = 'province_heat_map.html',\n",
    "                            legend_name = None):    \n",
    "    #preprocessing \n",
    "    province_data = province_data.dropna()\n",
    "    province_data[province_id_column_name]=\\\n",
    "        province_data[province_id_column_name].astype(int)\n",
    "    \n",
    "    province_geo_paths = get_province_geo_paths()\n",
    "    province_map = folium.Map([52, 19], zoom_start=6)\n",
    "    folium.Choropleth(geo_data=province_geo_paths,\n",
    "                  data=province_data,\n",
    "                    # kolumna z kluczem, kolumna z wartościami\n",
    "                  columns=[province_key_column_name, heat_column_name], \n",
    "                      # klucz z geoJSON\n",
    "                  key_on='feature.properties.JPT_KOD_JE', \n",
    "                  fill_color='YlOrRd', \n",
    "                  fill_opacity=0.7,\n",
    "                  line_opacity=0.2,\n",
    "                  legend_name=legend_name).add_to(province_map)\n",
    "    # zapisanie utworzonej mapy do pliku HTML\n",
    "    display(province_map)\n",
    "    province_map.save(outfile = file_path)\n",
    "    webbrowser.open(file_path) # Default browser open\n",
    "    \n",
    "def get_province_geo_paths():\n",
    "    province_shapes = gpd.read_file('wojewodztwa.shp')\n",
    "    province_shapes = province_shapes[['JPT_KOD_JE', \"geometry\"]]\n",
    "    province_shapes['JPT_KOD_JE']=province_shapes['JPT_KOD_JE'].astype(int)\n",
    "    \n",
    "    # uproszczenie geometrii (mniejsza wartosc = bardziej dokładnie)\n",
    "    province_shapes.geometry = province_shapes.geometry.simplify(0.005)\n",
    "    province_geo_path = province_shapes.to_json()\n",
    "    return province_geo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_city(data, address_column_name):\n",
    "    unique_places = data.groupby(address_column_name)[id_column_name].nunique()\n",
    "    places = pd.DataFrame({city_column_name:unique_places.index,\n",
    "                           numerical_column_name:unique_places.values})\n",
    "    places[lat_column_name]=np.nan\n",
    "    places[lon_column_name]=np.nan\n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print heat map with offer count\n",
    "grouped_city_data = group_by_city(df, city_column_name)\n",
    "places_map = fill_geo_info(grouped_city_data, city_column_name)\n",
    "print_heat_map(places_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Get province data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_province_ids():\n",
    "    return pd.read_csv('woj_oznaczenia.csv', engine='python')\n",
    "\n",
    "\n",
    "def get_city_data(data, address_column_name):\n",
    "    unique_places = data.groupby(address_column_name)[id_column_name].nunique()\n",
    "    \n",
    "    places = pd.DataFrame({city_column_name:unique_places.index})\n",
    "    places[lat_column_name]=np.nan\n",
    "    places[lon_column_name]=np.nan\n",
    "    places[province_column_name]=np.nan\n",
    "    \n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_map = get_city_data(df, city_column_name)\n",
    "places_map = fill_geo_info(places_map, city_column_name, fill_province=True)\n",
    "\n",
    "# jjit_data = jjit_data.drop([lat_column_name, lon_column_name, province_column_name])\n",
    "jjit_data = pd.merge(df, places_map, how='outer', \n",
    "                     left_on=city_column_name, right_on=city_column_name)\n",
    "\n",
    "province_ids = get_province_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Print heat map for min/max average salary </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by province and aggregate min salary\n",
    "min_salary_per_province_data = jjit_data.groupby(by=province_column_name) \\\n",
    "       .agg({'salary_from':'mean'}) \\\n",
    "       .rename(columns={'salary_from':'mean_salary_from'}) \\\n",
    "       .reset_index()\n",
    "\n",
    "#merge with province GUGiK data (we need province id)\n",
    "province_data = pd.merge(min_salary_per_province_data, province_ids, how='outer',\n",
    "                         left_on=province_column_name, right_on=province_column_name)\n",
    "\n",
    "print_province_heat_map(province_data = province_data,\n",
    "                        heat_column_name=\"mean_salary_from\",\n",
    "                        file_path=\"min.html\",\n",
    "                        legend_name=\"Średnie minimalne wynagrodzenia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by province and aggregate max salary\n",
    "max_salary_per_province_data = jjit_data.groupby(province_column_name) \\\n",
    "       .agg({'salary_to':'mean'}) \\\n",
    "       .rename(columns={'salary_to':'mean_salary_to'}) \\\n",
    "       .reset_index()\n",
    "\n",
    "#merge with province GUGiK data (we need province id)\n",
    "province_data = pd.merge(max_salary_per_province_data, province_ids, how='outer',\n",
    "                         left_on=province_column_name, right_on=province_column_name)\n",
    "\n",
    "print_province_heat_map(province_data = province_data,\n",
    "                        heat_column_name=\"mean_salary_to\",\n",
    "                        file_path=\"maks.html\",\n",
    "                        legend_name=\"Średnie maksymalne wynagrodzenia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# import difflib\n",
    "# # temp = dataset['skills'][0]\n",
    "# skills_arr = []\n",
    "# # skills = [skill_arr.append() for r in dataset['skills']]\n",
    "# for r in dataset['skills']:\n",
    "#     for name in r:\n",
    "#         skills_arr.append(name['name'])\n",
    "# unique_skills = np.unique(np.array(skills_arr))\n",
    "# ## usuwanie po znalezionych podobnych stwierdzeniach\n",
    "# similar = []\n",
    "# very_unique = []\n",
    "# for skill in unique_skills:\n",
    "#     if not skill in similar:\n",
    "#         very_unique.append(skill)\n",
    "#         similar += difflib.get_close_matches(skill, unique_skills, cutoff=0.1)\n",
    "\n",
    "# ## usuwanie po 1. słowie\n",
    "# first_word = []\n",
    "# very_very_unique = []\n",
    "# for skill in very_unique:\n",
    "#     if not skill.split(' ')[0] in first_word:\n",
    "#         first_word.append(skill.split(' ')[0])\n",
    "#         very_very_unique.append(skill)\n",
    "# print(len(very_unique))\n",
    "# pd.DataFrame(very_very_unique).to_csv('./data/skills_double_trim.csv')\n",
    "# # dataset_skills = read_json(dataset['skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df = df.copy()\n",
    "for col in [\"city\", \"Unnamed: 0\", \"title\", \"company_size\", \"country_code\", \"marker_icon\", \"company_name\",\n",
    "            \"latitude\", \"longitude\", \"salary_currency\", \"published_at\", \"remote_interview\", \"id\",\n",
    "            \"Vert.x\", \"skills\", \"-\"]:\n",
    "                cl_df=cl_df.drop(col, axis=1)\n",
    "print(cl_df.columns)\n",
    "cols_for_encoding = [c for c in cl_df.select_dtypes(include=['object']).copy().columns if c != \"experience_level\"]\n",
    "print(cols_for_encoding)\n",
    "cl_df = pd.get_dummies(cl_df, columns=cols_for_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_col_to_end(df, col):\n",
    "    cols_at_end = [col]\n",
    "    return df[[c for c in df if c not in cols_at_end] \n",
    "            + [c for c in cols_at_end if c in df]]\n",
    "\n",
    "cl_df = move_col_to_end(cl_df, \"experience_level\")\n",
    "array = cl_df.values\n",
    "x = array[:,0:len(cl_df.columns)-1]\n",
    "y = array[:,len(cl_df.columns)-1]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "#na podstawie x i y otrzymujemy tablice testowe i wynikowe\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x,y, test_size=cfg.test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.extend([\n",
    "    ('KNN', KNeighborsClassifier(), 0),\n",
    "    ('CART', DecisionTreeClassifier(), 1),\n",
    "    ('NB', GaussianNB(), 2),\n",
    "    ('SVM', SVC(gamma='auto'), 3),\n",
    "    ('MLP', MLPClassifier(alpha=1e-5, hidden_layer_sizes=(50,10), max_iter=5000), 4)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classification</h2>\n",
    "kfold - k cross-validation to algorytm polegający na testowaniu nauczania(sprawdzania jego wydajności). \n",
    "Zbiór TESTOWY jest dzielony na K podzbiorów. W każdej z k iteracji,\n",
    "brane jest k-1 pozdbiorów, następuje ich nauczanie, następnie sprawdzenie 'jakości' nauczonego modelu.\n",
    "Przy pomocy danego algorytmu uczenia maszynowego!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_show():\n",
    "    pyplot.draw()\n",
    "    pyplot.pause(0.1)\n",
    "\n",
    "def get_specificity(y_validate, y_predicted):\n",
    "    cnf_matrix = confusion_matrix(y_validate, y_predicted)\n",
    "    \n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    \n",
    "    FP = FP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "        \n",
    "    return np.mean(TN/(TN+FP))\n",
    "    \n",
    "def get_learning_curve(classification_model, training_set_enlarging_step=10):\n",
    "    train_sizes = np.linspace(0.1, 1, training_set_enlarging_step)\n",
    "    train_sizes, train_scores, validation_scores = learning_curve(\n",
    "        estimator = classification_model,\n",
    "        X = x,\n",
    "        y = y, \n",
    "        train_sizes = train_sizes,\n",
    "        cv = 5,\n",
    "        scoring = 'accuracy')\n",
    "    return train_sizes, train_scores, validation_scores\n",
    "\n",
    "def plot_learning_curve(model,name):\n",
    "    train_sizes, train_scores, test_scores = \\\n",
    "        get_learning_curve(model)\n",
    "\n",
    "    plt.style.use('seaborn')\n",
    "    plt.plot(train_sizes, -train_scores.mean(axis = 1), color= 'red', label = 'Training error')\n",
    "    plt.plot(train_sizes, -test_scores.mean(axis = 1), color= 'navy',label = 'Validation error')\n",
    "    plt.ylabel('Accuracy', fontsize = 14)\n",
    "    plt.xlabel('Training set size', fontsize = 14)\n",
    "    plt.title('Learning curves for a %s' % name, fontsize = 18, y = 1.03)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def compare_algorithms(results, names):\n",
    "    fig = pyplot.figure()\n",
    "    fig.suptitle(\"Algorithm Comparison\")\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_title(\"Algorithm Comparison\")\n",
    "    ax.boxplot(results, labels=names)\n",
    "    plot_show()\n",
    "\n",
    "def print_scores(cv_results, predictions):\n",
    "    print('\\nMean %f' % cv_results.mean())\n",
    "    print('STD %f' % cv_results.std())\n",
    "    print('\\nConfusion matrix:')\n",
    "    print(confusion_matrix(y_validation, predictions))\n",
    "    print('\\nAccuracy %f' % accuracy_score(y_validation, predictions))\n",
    "    print('Precision %f' % precision_score(y_validation, predictions, average = 'weighted'))\n",
    "    print('Recall %f' % recall_score(y_validation, predictions, average = 'weighted'))\n",
    "    print('Specificity %f' % get_specificity(y_validation, predictions))\n",
    "    print('\\nClassification report:')\n",
    "    print(classification_report(y_validation, predictions))\n",
    "\n",
    "def plot_roc_curves():\n",
    "    fig = pyplot.figure()\n",
    "    ax = plt.gca()\n",
    "    for name, model, subplot_row in models:\n",
    "        rfc_disp = plot_roc_curve(model, x_validation, \n",
    "                                  y_validation, ax=ax, alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "def accuracySignificancy(model, x_train, y_train, cv):\n",
    "    fig = pyplot.figure()\n",
    "    fig.suptitle(\"Estimating accuracy score's statistical significancy\")\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    n_classes = np.unique(y_train).size\n",
    "    score, permutation_scores, pvalue = permutation_test_score(model, x_train, y_train, scoring=\"accuracy\", cv=cv, n_permutations=100)\n",
    "    print(\"Classification score %s (pvalue : %s)\" % (score, pvalue))\n",
    "    # View histogram of permutation scores\n",
    "    ax.hist(permutation_scores, 20, label='Permutation scores',\n",
    "             edgecolor='black')\n",
    "    ylim = plt.ylim()\n",
    "    ax.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "             label='Classification Score'\n",
    "             ' (pvalue %s)' % pvalue)\n",
    "    ax.plot(2 * [1. / n_classes], plt.ylim(), '--k', linewidth=3, label='Luck')\n",
    "    ax.set_xlabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "def clustering_scores(x, labels_true=None):\n",
    "    inertias = []\n",
    "    silhouette_values = []\n",
    "    calinski_harabasz = []\n",
    "    davies_bouldin = []\n",
    "    k_range = 15\n",
    "    for k in range(2, k_range):\n",
    "        model = KMeans(n_clusters=k, random_state=1)\n",
    "        clustering = model.fit(x)\n",
    "        labels=clustering.labels_\n",
    "        inertias.append(clustering.inertia_)\n",
    "        silhouette_values.append(silhouette_score(x, labels))\n",
    "        calinski_harabasz.append(calinski_harabasz_score(x, labels))\n",
    "        davies_bouldin.append(davies_bouldin_score(x, labels))\n",
    "\n",
    "    fig, ax = plt.subplots(4, 1, figsize=(8,32))\n",
    "    ax[0].plot(inertias)\n",
    "    ax[0].set_title(\"Elbow chart\")\n",
    "    ax[0].set_xlabel('clusters')\n",
    "    ax[0].set_ylabel('distortion')\n",
    "    ax[1].plot(silhouette_values)\n",
    "    ax[1].set_title(\"Silhouette score\")\n",
    "    ax[1].set_xlabel('clusters')\n",
    "    ax[1].set_ylabel('Silhouette score')\n",
    "    ax[2].plot(calinski_harabasz)\n",
    "    ax[2].set_title(\"Calinski Harabasz score\")\n",
    "    ax[2].set_xlabel('clusters')\n",
    "    ax[2].set_ylabel('Calinski Harabasz score')\n",
    "    ax[3].plot(davies_bouldin)\n",
    "    ax[3].set_title(\"Davies Bouldin score\")\n",
    "    ax[3].set_xlabel('clusters')\n",
    "    ax[3].set_ylabel('Davies Bouldin score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_per_dataset():\n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model, subplot_row in models:\n",
    "            print(f\"---------------------------\\nRunning classification for: {name}\")\n",
    "#             kfold = StratifiedKFold(n_splits=cfg.n_splits, random_state=1, shuffle=True)\n",
    "            kfold = StratifiedKFold(n_splits=4, random_state=1, shuffle=True)\n",
    "            cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "            accuracySignificancy(model, x_train, y_train, kfold)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "\n",
    "            # Make predictions on validation dataset\n",
    "            model.fit(x_train, y_train)\n",
    "            predictions = model.predict(x_validation)\n",
    "            \n",
    "            print_scores(cv_results, predictions)\n",
    "            plot_learning_curve(model,name)\n",
    "    #ROC\n",
    "    plot_roc_curves()\n",
    "\n",
    "    # Compare Algorithms - ROC etc\n",
    "    compare_algorithms(results, names)\n",
    "    \n",
    "    clustering_scores(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_per_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
